services:
  # 1. Our FastAPI Application (No changes here)
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ingestiq_api
    ports:
      - "8001:8000"
    volumes:
      - ./app:/app
      - ./scripts:/app/scripts
    env_file: .env
    environment:
      - PYTHONPATH=/app
    depends_on:
      - postgres
      - rabbitmq
    command: sh -c "cd /app && python -m uvicorn main_api:app --host 0.0.0.0 --port 8000"

  # 2. PostgreSQL for Metadata & State (No changes here)
  postgres:
    image: postgres:16-alpine
    container_name: ingestiq_postgres
    ports:
      - "5432:5432"
    volumes:
      - ./local_data/postgres_data:/var/lib/postgresql/data
    env_file: .env
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}"]
      interval: 5s
      timeout: 5s
      retries: 5

  # 4. RabbitMQ for Messaging (No changes here)
  rabbitmq:
    image: rabbitmq:3-management-alpine
    container_name: ingestiq_rabbitmq
    ports:
      - "5672:5672"
      - "15672:15672"
    env_file: .env
    healthcheck:
      test: ["CMD-SHELL", "rabbitmq-diagnostics -q check_running"]
      interval: 5s
      timeout: 5s
      retries: 5

  # 5. ChromaDB for Vector Storage (No changes here)
  chroma:
    image: chromadb/chroma:0.5.0
    container_name: ingestiq_chroma
    ports:
      - "8000:8000"
    volumes:
      - ./local_data/chroma_data:/chroma/.chroma/index

  # --- SIMPLIFIED AND CORRECTED AIRFLOW SERVICES ---

  airflow-webserver:
    build:
      context: . # Build context is the project root
      dockerfile: airflow/Dockerfile # Use our new, unified Dockerfile
    container_name: ingestiq_airflow_webserver
    ports:
      - "8080:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./app:/opt/airflow/app # Mount our app code
      - ./local_data:/opt/airflow/local_data
    env_file: .env
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - AIRFLOW__CELERY__BROKER_URL=amqp://${RABBITMQ_DEFAULT_USER}:${RABBITMQ_DEFAULT_PASS}@rabbitmq:5672/
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__WEBSERVER__SECRET_KEY=${SECRET_KEY}
      - PYTHONPATH=/opt/airflow/app
    depends_on:
      postgres:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
      chroma:
        condition: service_started
    command: webserver

  airflow-scheduler:
    build:
      context: .
      dockerfile: airflow/Dockerfile
    container_name: ingestiq_airflow_scheduler
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./app:/opt/airflow/app
      - ./local_data:/opt/airflow/local_data
    env_file: .env
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - AIRFLOW__CELERY__BROKER_URL=amqp://${RABBITMQ_DEFAULT_USER}:${RABBITMQ_DEFAULT_PASS}@rabbitmq:5672/
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__WEBSERVER__SECRET_KEY=${SECRET_KEY}
      - PYTHONPATH=/opt/airflow/app
    depends_on:
      postgres:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
      chroma:
        condition: service_started
    command: scheduler

    # Removed worker as LocalExecutor handled it

  # This service runs once to initialize the Airflow database
  airflow-init:
    build:
      context: .
      dockerfile: airflow/Dockerfile
    container_name: ingestiq_airflow_init
    env_file: .env
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - AIRFLOW__CELERY__BROKER_URL=amqp://${RABBITMQ_DEFAULT_USER}:${RABBITMQ_DEFAULT_PASS}@rabbitmq:5672/
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - PYTHONPATH=/opt/airflow/app
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./app:/opt/airflow/app
      - ./local_data:/opt/airflow/local_data
    depends_on:
      - postgres
      - rabbitmq
    command: >
      bash -c "
        airflow db init &&
        airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin &&
        echo 'Waiting for DAG to be discovered...' &&
        for i in {1..30}; do
          if airflow dags list | grep -q ingestion_pipeline; then
            airflow dags unpause ingestion_pipeline && exit 0
          fi
          sleep 2
        done
        echo 'Timeout waiting for DAG discovery' && exit 1
      "

volumes:
  postgres_data:
